{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ccc742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer and models\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(DEVICE)\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, \"./tinyllama_lora7_final\").to(DEVICE)\n",
    "base_model.eval()\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files={\"test\": \"test.jsonl\"}, split=\"test\", encoding=\"utf-8\")\n",
    "\n",
    "# Prompt formatting\n",
    "def format_prompt(instruction):\n",
    "    return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "\n",
    "# Perplexity calculation\n",
    "@torch.no_grad()\n",
    "def compute_perplexity(model, dataset, tokenizer, max_length=512):\n",
    "    total_loss, total_tokens = 0, 0\n",
    "    for example in tqdm(dataset, desc=\"Computing Perplexity\"):\n",
    "        prompt = format_prompt(example[\"instruction\"]) + example[\"output\"]\n",
    "        inputs = tokenizer(prompt, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\").to(DEVICE)\n",
    "        labels = inputs.input_ids.clone()\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        loss = model(**inputs, labels=labels).loss.item()\n",
    "        valid_tokens = (labels != -100).sum().item()\n",
    "        total_loss += loss * valid_tokens\n",
    "        total_tokens += valid_tokens\n",
    "    return np.exp(total_loss / total_tokens)\n",
    "\n",
    "# Response generation\n",
    "def generate_response(model, instruction, max_new_tokens=200):\n",
    "    prompt = format_prompt(instruction)\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(DEVICE)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, top_p=0.9, temperature=0.6)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"### Response:\\n\")[-1].strip()\n",
    "    except Exception:\n",
    "        return \"[ERROR: Failed to generate response]\"\n",
    "\n",
    "# Evaluation metrics\n",
    "def compute_metrics(reference, hypothesis, rouge):\n",
    "    bleu = sentence_bleu([reference.split()], hypothesis.split(), smoothing_function=SmoothingFunction().method1)\n",
    "    rouge_scores = rouge.score(reference, hypothesis)\n",
    "    P, R, F1 = score([hypothesis], [reference], lang=\"en\", model_type=\"microsoft/deberta-xlarge-mnli\", verbose=False)\n",
    "    return bleu, rouge_scores, F1.item()\n",
    "\n",
    "# Evaluation process\n",
    "results = {\"base\": {}, \"fine_tuned\": {}}\n",
    "results[\"base\"][\"perplexity\"] = compute_perplexity(base_model, dataset, tokenizer)\n",
    "results[\"fine_tuned\"][\"perplexity\"] = compute_perplexity(fine_tuned_model, dataset, tokenizer)\n",
    "\n",
    "# Containers for metrics\n",
    "bleu_scores = {\"base\": [], \"fine_tuned\": []}\n",
    "rouge_scores = {\"base\": {\"rouge1\": [], \"rougeL\": []}, \"fine_tuned\": {\"rouge1\": [], \"rougeL\": []}}\n",
    "bert_scores = {\"base\": [], \"fine_tuned\": []}\n",
    "rouge = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "# Main evaluation loop\n",
    "for example in tqdm(dataset, desc=\"Evaluating Responses\"):\n",
    "    instruction, reference = example[\"instruction\"], example[\"output\"]\n",
    "    base_response = generate_response(base_model, instruction)\n",
    "    ft_response = generate_response(fine_tuned_model, instruction)\n",
    "\n",
    "    base_bleu, base_rouge, base_bert = compute_metrics(reference, base_response, rouge)\n",
    "    ft_bleu, ft_rouge, ft_bert = compute_metrics(reference, ft_response, rouge)\n",
    "\n",
    "    bleu_scores[\"base\"].append(base_bleu)\n",
    "    bleu_scores[\"fine_tuned\"].append(ft_bleu)\n",
    "    for key in [\"rouge1\", \"rougeL\"]:\n",
    "        rouge_scores[\"base\"][key].append(base_rouge[key].fmeasure)\n",
    "        rouge_scores[\"fine_tuned\"][key].append(ft_rouge[key].fmeasure)\n",
    "    bert_scores[\"base\"].append(base_bert)\n",
    "    bert_scores[\"fine_tuned\"].append(ft_bert)\n",
    "\n",
    "# Aggregate scores\n",
    "for key in [\"bleu\", \"rouge1\", \"rougeL\", \"bertscore\"]:\n",
    "    if key == \"bleu\":\n",
    "        results[\"base\"][key] = np.mean(bleu_scores[\"base\"])\n",
    "        results[\"fine_tuned\"][key] = np.mean(bleu_scores[\"fine_tuned\"])\n",
    "    elif key.startswith(\"rouge\"):\n",
    "        results[\"base\"][key] = np.mean(rouge_scores[\"base\"][key])\n",
    "        results[\"fine_tuned\"][key] = np.mean(rouge_scores[\"fine_tuned\"][key])\n",
    "    elif key == \"bertscore\":\n",
    "        results[\"base\"][key] = np.mean(bert_scores[\"base\"])\n",
    "        results[\"fine_tuned\"][key] = np.mean(bert_scores[\"fine_tuned\"])\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nQuantitative Evaluation Results:\")\n",
    "for model in [\"base\", \"fine_tuned\"]:\n",
    "    print(f\"{model.title()} Model:\")\n",
    "    for metric in results[model]:\n",
    "        print(f\"  {metric.capitalize()}: {results[model][metric]:.4f}\")\n",
    "\n",
    "# Paired t-tests\n",
    "print(\"\\nStatistical Significance (Paired t-tests):\")\n",
    "t_bleu, p_bleu = ttest_rel(bleu_scores[\"base\"], bleu_scores[\"fine_tuned\"])\n",
    "print(f\"BLEU p-value: {p_bleu:.4f}\")\n",
    "t_rouge1, p_rouge1 = ttest_rel(rouge_scores[\"base\"][\"rouge1\"], rouge_scores[\"fine_tuned\"][\"rouge1\"])\n",
    "print(f\"ROUGE-1 p-value: {p_rouge1:.4f}\")\n",
    "t_bert, p_bert = ttest_rel(bert_scores[\"base\"], bert_scores[\"fine_tuned\"])\n",
    "print(f\"BERTScore p-value: {p_bert:.4f}\")\n",
    "\n",
    "# Robustness Test (Out-of-Domain Prompts)\n",
    "print(\"\\nRobustness Test (Out-of-Domain Instructions):\")\n",
    "ood_questions = [\n",
    "    \"How can CSS be optimized to reduce carbon emissions?\",\n",
    "    \"How does video loading impact a webpage's energy consumption?\",\n",
    "    \"What is a carbon footprint, and how can it be reduced in web design?\"\n",
    "]\n",
    "for i, q in enumerate(ood_questions):\n",
    "    base = generate_response(base_model, q)\n",
    "    ft = generate_response(fine_tuned_model, q)\n",
    "    print(f\"\\nQuestion {i+1}: {q}\")\n",
    "    print(f\"Base: {base}\")\n",
    "    print(f\"Fine-tuned: {ft}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
