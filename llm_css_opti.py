import os
import argparse
import cssutils
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableSequence
from langchain_huggingface import HuggingFacePipeline

MAX_TOKENS_PER_CHUNK = 1800


def extract_css_blocks(css_text):
    cssutils.log.setLevel('FATAL')
    sheet = cssutils.parseString(css_text, validate=False)
    blocks = []
    for rule in sheet:
        if rule.type == rule.STYLE_RULE:
            block_text = str(rule.cssText)
            blocks.append(block_text)
    return blocks


def group_blocks_by_token_limit(blocks, tokenizer, max_tokens):
    chunks = []
    current_chunk = ""
    current_tokens = 0

    for block in blocks:
        block_tokens = len(tokenizer.encode(block))
        if current_tokens + block_tokens > max_tokens:
            chunks.append(current_chunk.strip())
            current_chunk = block
            current_tokens = block_tokens
        else:
            current_chunk += "\n\n" + block
            current_tokens += block_tokens

    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks


def run_llm_safe(project_name):
    group = "llm"
    input_path = os.path.join("css_optimizer", "css_original", project_name, "style.css")
    output_dir = os.path.join("css_optimizer", "css_optimized", project_name, group)
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, "style.css")

    if not os.path.exists(input_path):
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_path}")
        return

    with open(input_path, "r", encoding="utf-8") as f:
        css_code = f.read()

    model_name = "deepseek-ai/deepseek-coder-6.7b-instruct"
    print("ğŸ§  æ­£åœ¨åŠ è½½ DeepSeek æ¨¡å‹...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype="auto")
    except Exception as e:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥:", e)
        return

    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=1024,
        do_sample=False,
        return_full_text=False,
    )
    llm = HuggingFacePipeline(pipeline=pipe)

    # âœ… é«˜è´¨é‡ Promptï¼ˆé¿å…çåˆ /é‡æ’ï¼‰
    prompt_template = PromptTemplate(
        input_variables=["css_code"],
        template=(
            "ä½ æ˜¯ä¸€ä¸ªå‰ç«¯æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ã€‚è¯·ä¼˜åŒ–ä»¥ä¸‹ CSS ä»£ç ï¼Œç›®æ ‡ï¼š\n"
            "- åˆ é™¤æ— æ•ˆæˆ–é‡å¤æ ·å¼\n"
            "- åˆå¹¶é‡å¤å£°æ˜\n"
            "- ä¿æŒè§†è§‰ä¸€è‡´æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§\n"
            "- ä¸æ›´æ”¹é€‰æ‹©å™¨åç§°ã€é¡ºåºæˆ–åµŒå¥—ç»“æ„\n"
            "- ä¸åˆ é™¤ä»»ä½•è§„åˆ™ï¼Œé™¤éä¸å…¶ä»–è§„åˆ™å®Œå…¨é‡å¤\n"
            "- ä¸å°†æ ·å¼å‹ç¼©æˆä¸€è¡Œï¼Œä¿ç•™æ ¼å¼å¯è¯»æ€§\n\n"
            "{css_code}"
        )
    )

    chain: RunnableSequence = prompt_template | llm

    blocks = extract_css_blocks(css_code)
    chunks = group_blocks_by_token_limit(blocks, tokenizer, MAX_TOKENS_PER_CHUNK)

    print(f"ğŸ” å‘ç° {len(blocks)} ä¸ª CSS è§„åˆ™å—ï¼Œåˆ†ä¸º {len(chunks)} ä¸ª LLM è¾“å…¥å—")

    optimized_css_chunks = []

    for i, chunk in enumerate(chunks):
        print(f"ğŸ§© æ­£åœ¨ä¼˜åŒ–ç¬¬ {i+1}/{len(chunks)} å—...")
        print(f"   â¤· chunk å­—ç¬¦æ•°: {len(chunk)}, token æ•°: {len(tokenizer.encode(chunk))}")
        try:
            result = chain.invoke({"css_code": chunk})
            optimized_css_chunks.append(result.strip())
        except Exception as e:
            print(f"âŒ ç¬¬ {i+1} å—ä¼˜åŒ–å¤±è´¥:", e)

    final_css = "\n\n".join(optimized_css_chunks)

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(final_css)

    print(f"ğŸ“Š ä¼˜åŒ–å‰è¡Œæ•°: {len(css_code.splitlines())}")
    print(f"ğŸ“Š ä¼˜åŒ–åè¡Œæ•°: {len(final_css.splitlines())}")
    print(f"âœ… æ‰€æœ‰å—ä¼˜åŒ–å®Œæˆï¼Œç»“æœä¿å­˜åˆ°ï¼š{output_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç»“æ„å®‰å…¨çš„ LLM CSS ä¼˜åŒ–ï¼ˆé˜²æ­¢çåˆ  + æ‰“å°åˆ†æï¼‰")
    parser.add_argument("project_name", help="é¡¹ç›®åï¼ˆå¦‚ site3ï¼‰")
    args = parser.parse_args()

    run_llm_safe(args.project_name)

